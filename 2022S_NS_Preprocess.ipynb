{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import & Load the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2536ndIxYAIz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wyUKhrKnYwyT"
   },
   "outputs": [],
   "source": [
    "# Load the raw log data\n",
    "raw = pd.read_csv(os.path.join(os.getcwd(),'log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5948704, 7)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_mskhaq1ZV0s"
   },
   "source": [
    "# 1. Basic Preprocess"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Merge the Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e47jSQ2pZL77"
   },
   "outputs": [],
   "source": [
    "# Load the pre-defined label list\n",
    "labels = pd.read_csv(os.path.join(os.getcwd(),'label.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UaQvNWYBZtHA"
   },
   "outputs": [],
   "source": [
    "# Convert the data type of \"datetime\" column\n",
    "# Extract integral parts from the \"page\" column and save it as the \"PAGE\" column (in order to match the column name with the pre-defined label.csv file)\n",
    "raw['datetime'] = pd.to_datetime(raw['datetime'])\n",
    "raw['PAGE'] = raw['page'].apply(lambda x: str(x.split('/')[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xz-kEXVNaZc5"
   },
   "outputs": [],
   "source": [
    "# Merge the pre-defined label data and the raw log.csv\n",
    "raw = raw.merge(labels,on='PAGE',how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Converting the data types of columns \"datetime\", \"page\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "MBxYfANOa0mc"
   },
   "outputs": [],
   "source": [
    "# Convert the data type of \"datetime\" column (once again)\n",
    "# Drop the useless columns (\"page\", \"Num\")\n",
    "raw['datetime'] = pd.to_datetime(raw['datetime'])\n",
    "raw.drop(['page','Num','URL'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "93b2BESHbSqJ"
   },
   "outputs": [],
   "source": [
    "# Change the name of column \"PAGE\" into \"page\" (for convenience)\n",
    "# Sort the columns \"user_id\" and \"datetime\" by an ascending order\n",
    "raw.rename(columns={'PAGE':'page'},inplace=True)\n",
    "raw.sort_values(['user_id','datetime'],inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Remove the outlier logs & Split the sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "1t0VWEIDbqZf"
   },
   "outputs": [],
   "source": [
    "# Remove the logs labels as \"*\"\n",
    "raw = raw[raw['Labeling']!='*'].reset_index().drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since only numeric labels are left, convert the data type of \"Labeling\" column into int\n",
    "raw['Labeling'] = raw['Labeling'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lgPBf9lDcB9A"
   },
   "outputs": [],
   "source": [
    "# Divide the session if the difference between two adjacent logs are larger than 60 minutes\n",
    "# Create a new \"id\" column by combining the \"user_id\" column and the session number\n",
    "raw['session'] = raw.groupby('user_id')['datetime'].apply(lambda x: x.diff().gt('60Min').cumsum())\n",
    "raw['id'] = raw['user_id']+'_'+raw['session'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the column \"page_stay_time(how long did a user stayed in the page)\"\n",
    "raw['page_stay_time'] = raw.groupby('id')['datetime'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the logs that have the duplicated URL with the previous log\n",
    "raw['prev_url'] = raw.groupby('id')['url'].shift(1)\n",
    "raw = raw[raw['url']!=raw['prev_url']].drop('prev_url',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5394352, 11)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Create basic featueres for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QSOyalfEcldU"
   },
   "outputs": [],
   "source": [
    "# Create the \"session_length\" column\n",
    "# Create the \"session_nth(nth log ion the session)\" column\n",
    "raw = raw.merge(pd.DataFrame(raw.groupby('id')['session'].count()).rename(columns={'session':'session_length'}),on='id',how='left')\n",
    "raw['session_nth'] = raw.groupby('id').cumcount()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9k3rHRsFdYLK"
   },
   "outputs": [],
   "source": [
    "# Create the \"prev_Labeling(previous label within the session)\" column\n",
    "# Create the \"prev_page(previous page within the session)\" column\n",
    "raw['prev_Labeling'] = raw.groupby('id')['Labeling'].shift(1)\n",
    "raw['prev_page'] = raw.groupby('id')['page'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ddLBOkr1eDLX"
   },
   "outputs": [],
   "source": [
    "# Create the \"begin_time(beginning timestamp of the session)\" column\n",
    "# Create the \"end_time(ending timestamp of the session)\" column\n",
    "raw = raw.merge(pd.DataFrame(raw.groupby('id')['datetime'].min()).rename(columns={'datetime':'begin_time'}),on='id',how='left')\n",
    "raw = raw.merge(pd.DataFrame(raw.groupby('id')['datetime'].max()).rename(columns={'datetime':'end_time'}),on='id',how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HqjJDScTe7JF"
   },
   "source": [
    "# 2. Merge the Log&Label data with Cart, Like, Purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Eeso8OorfAtk",
    "outputId": "af674b5c-a6f0-4825-c987-17c40d686a4a"
   },
   "outputs": [],
   "source": [
    "# Load the raw Cart, Like, Purchase data\n",
    "cart = pd.read_csv(os.path.join(os.getcwd(),'cart.csv'))\n",
    "like = pd.read_csv(os.path.join(os.getcwd(),'like.csv'))\n",
    "purchase = pd.read_csv(os.path.join(os.getcwd(),'purchase.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(212770, 8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92147, 9)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "like.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(123985, 11)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purchase.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_FEIker-fMCw"
   },
   "outputs": [],
   "source": [
    "# For the merging task, create the \"end_time2(60 minutes added to the ending timestamp of the session)\" column\n",
    "raw['end_time2'] = raw['end_time'] + pd.to_timedelta(1, unit='h')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ajT2Sq1Yf1cZ"
   },
   "source": [
    "## 2.1. Merge with the Cart data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "gtNqhm2xiSSu"
   },
   "source": [
    "> If there exists an action of the particular user in Cart.csv, check if any of that user's session (defined by the log data) contains the action's timestamp\n",
    "\n",
    "\n",
    "\n",
    "> If such session session exists, match the action in Cart.csv to the nearest (in time) log in that session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NI_Fbmv-frD9",
    "outputId": "3e4650ab-b9fe-4c24-8af2-a717d334bdad"
   },
   "outputs": [],
   "source": [
    "# Sort the Cart data by user_id and timestamp\n",
    "# For the merging task, round the timestamp in Cart data up to a second\n",
    "# Since Cart data is recorded based on UTC+9(hour) system, substract 9 hours in order to match the timestamp of Raw data\n",
    "# Extract integral part from the \"event_type\" column of the Cart data\n",
    "\n",
    "cart.sort_values(by=['user_id','event_ymdt'],inplace=True)\n",
    "cart['event_ymdt'] = pd.to_datetime(cart['event_ymdt']).round('s')\n",
    "cart['event_ymdt'] = cart['event_ymdt'] - timedelta(hours=9)   \n",
    "cart['event_type'] = cart['event_type'].apply(lambda x: x.split('.')[-1])\n",
    "cart.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "a25DpMa6gecZ"
   },
   "outputs": [],
   "source": [
    "# Merge the Cart data if the timestamp of user's action lies between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "# It is highly inefficient to compare all the timestamps of Raw data and Cart data\n",
    "# Comparing only the timestamps of the same date is much more efficient, thus create \"date\" column in the Raw and Cart data\n",
    "# But in this case, problem arises when the session's ending time lies betweeen 23:00 and 00:00, and the timestamp of the action in the Cart data lies between 00:00 and 01:00\n",
    "# Therefore we create \"date2(1 day substractd from the date of the Cart data)\" column in the Cart data\n",
    "\n",
    "raw['date'] = raw['datetime'].dt.floor('d')\n",
    "cart['date'] = cart['event_ymdt'].dt.floor('d')\n",
    "cart['date2'] = cart['event_ymdt'].dt.floor('d') - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xIq36ebUgplA",
    "outputId": "9ad4ed2c-85da-422c-ccd5-d90be15177a7"
   },
   "outputs": [],
   "source": [
    "# Merge Raw and Cart data if \"date\" column coincides\n",
    "# Remove data if the timestamp of the user's action does not lie between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "log_cart = raw.merge(cart,on=['user_id','date'],how='inner')\n",
    "log_cart = log_cart[((log_cart['event_ymdt'] - log_cart['begin_time'])>timedelta(0))*((log_cart['end_time2']-log_cart['event_ymdt'])>timedelta(0))==1]\n",
    "log_cart.sort_values(by=['id','datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Raw and Cart data if \"date\" column of Raw data and \"date2(1 day substractd from the date of the Cart data)\" column of Cart data coincides\n",
    "# Remove data if the timestamp of the user's action does not lie between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "log_cart2 = raw.merge(cart,left_on=['user_id','date'],right_on=['user_id','date2'],how='inner')\n",
    "log_cart2 = log_cart2[((log_cart2['event_ymdt'] - log_cart2['begin_time'])>timedelta(0))*((log_cart2['end_time2']-log_cart2['event_ymdt'])>timedelta(0))==1]\n",
    "log_cart2.sort_values(by=['id','datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate above two data\n",
    "log_cart = pd.concat([log_cart,log_cart2],axis=0)\n",
    "\n",
    "# Match the action in the Cart data to the nearest (in time) log within the session\n",
    "log_cart['diff'] = ((log_cart['event_ymdt'] - log_cart['datetime']).astype('timedelta64[s]'))**2\n",
    "log_cart = log_cart.merge(pd.DataFrame(log_cart.groupby('id')['diff'].min()).rename(columns={'diff':'min_diff'}),on='id',how='left')\n",
    "log_cart = log_cart[(log_cart['diff']-log_cart['min_diff'])**2<0.1]\n",
    "log_cart.drop_duplicates(subset=['id','event_ymdt'],keep='last',inplace=True)\n",
    "log_cart.drop('min_diff',axis=1,inplace=True)\n",
    "log_cart['cart_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2997509047328101"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only 30% of the Cart data are matched to the log data \n",
    "# Unmatched data are estimated to be the actions on the web environment (we only used the log data from the mobile environment)\n",
    "log_cart.shape[0] / cart.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that were created but no longer useful\n",
    "# Add the suffix \"cart_\" to the columns created in this section, to avoid the ambiguity\n",
    "\n",
    "log_cart.drop(['cart_id','end_time2','date','date_x','date_y','date2'],axis=1,inplace=True)\n",
    "log_cart.rename(columns={'prod_no':'cart_prod_no','prod_nm':'cart_prod_nm','event_type':'cart_event_type','event_ymdt':'cart_event_ymdt',\n",
    "                                    'catg_nm_l':'cart_catg_nm_l','catg_nm_m':'cart_catg_nm_m','catg_nm_s':'cart_catg_nm_s','diff':'log_cart_time_diff'},inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Merge with the Like data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If there exists an action of the particular user in Like.csv, check if any of that user's session (defined by the log data) contains the action's timestamp\n",
    "\n",
    "\n",
    "\n",
    "> If such session session exists, match the action in Like.csv to the nearest (in time) log in that session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the Cart data by user_id and timestamp\n",
    "# Since Like data is recorded based on UTC system, we do not need to substract 9 hours\n",
    "# Drop the useless column (dt)\n",
    "\n",
    "like.sort_values(by=['user_id','keep_reg_ymdt'],inplace=True)\n",
    "like['keep_reg_ymdt'] = pd.to_datetime(like['keep_reg_ymdt'])\n",
    "like.drop('dt',axis=1,inplace=True)\n",
    "like.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Like data if the timestamp of user's action lies between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "# It is highly inefficient to compare all the timestamps of Raw data and Like data\n",
    "# Comparing only the timestamps of the same date is much more efficient, thus create \"date\" column in the Raw and Like data\n",
    "# But in this case, problem arises when the session's ending time lies betweeen 23:00 and 00:00, and the timestamp of the action in the Like data lies between 00:00 and 01:00\n",
    "# Therefore we create \"date2(1 day substractd from the date of the Like data)\" column in the Like data\n",
    "\n",
    "raw['date'] = raw['datetime'].dt.floor('d')\n",
    "like['date'] = like['keep_reg_ymdt'].dt.floor('d')\n",
    "like['date2'] = like['keep_reg_ymdt'].dt.floor('d') - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Raw and Like data if \"date\" column coincides\n",
    "# Remove data if the timestamp of the user's action does not lie between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "log_like = raw.merge(like,on=['user_id','date'],how='inner')\n",
    "log_like = log_like[((log_like['keep_reg_ymdt'] - log_like['begin_time'])>timedelta(0))*((log_like['end_time2']-log_like['keep_reg_ymdt'])>timedelta(0))==1]\n",
    "log_like.sort_values(by=['id','datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Raw and Like data if \"date\" column of Raw data and \"date2(1 day substractd from the date of the Cart data)\" column of Like data coincides\n",
    "# Remove data if the timestamp of the user's action does not lie between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "log_like2 = raw.merge(like,left_on=['user_id','date'],right_on=['user_id','date2'],how='inner')\n",
    "log_like2 = log_like2[((log_like2['keep_reg_ymdt'] - log_like2['begin_time'])>timedelta(0))*((log_like2['end_time2']-log_like2['keep_reg_ymdt'])>timedelta(0))==1]\n",
    "log_like2.sort_values(by=['id','datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate above two data\n",
    "log_like = pd.concat([log_like,log_like2],axis=0)\n",
    "\n",
    "# Match the action in the Like data to the nearest (in time) log within the session\n",
    "log_like['diff'] = ((log_like['keep_reg_ymdt'] - log_like['datetime']).astype('timedelta64[s]'))**2\n",
    "log_like = log_like.merge(pd.DataFrame(log_like.groupby('id')['diff'].min()).rename(columns={'diff':'min_diff'}),on='id',how='left')\n",
    "log_like = log_like[(log_like['diff']-log_like['min_diff'])**2<0.1]\n",
    "log_like.drop_duplicates(subset=['id','keep_reg_ymdt'],keep='last',inplace=True)\n",
    "log_like.drop('min_diff',axis=1,inplace=True)\n",
    "log_like['like_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2465842621029442"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only 24% of the Like data are matched to the log data \n",
    "# Unmatched data are estimated to be the actions on the web environment (we only used the log data from the mobile environment)\n",
    "\n",
    "log_like.shape[0] / like.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that were created but no longer useful\n",
    "# Add the suffix \"like_\" to the columns created in this section, to avoid the ambiguity\n",
    "\n",
    "log_like.drop(['like_id','end_time2','date','date2','date_x','date_y'],axis=1,inplace=True)\n",
    "log_like.rename(columns={'keep_stat_cd':'like_keep_stat_cd','prod_no':'like_prod_no','prod_nm':'like_prod_nm','keep_reg_ymdt':'like_keep_reg_ymdt',\n",
    "                                    'catg_nm_l':'like_catg_nm_l','catg_nm_m':'like_catg_nm_m','catg_nm_s':'like_catg_nm_s','diff':'log_like_time_diff'},inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Merge with the Purchase data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If there exists an action of the particular user in Purchase.csv, check if any of that user's session (defined by the log data) contains the action's timestamp\n",
    "\n",
    "\n",
    "\n",
    "> If such session session exists, match the action in Purchase.csv to the nearest (in time) log in that session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the Purchase data by user_id and timestamp\n",
    "# Since Purchase data is recorded based on UTC system, we do not need to substract 9 hours\n",
    "\n",
    "purchase.sort_values(by=['user_id','order_ymdt'],inplace=True)\n",
    "purchase['order_ymdt'] = pd.to_datetime(purchase['order_ymdt'].apply(lambda x: '20'+x))\n",
    "purchase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the Purchase data if the timestamp of user's action lies between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "# It is highly inefficient to compare all the timestamps of Raw data and Purchase data\n",
    "# Comparing only the timestamps of the same date is much more efficient, thus create \"date\" column in the Raw and Purchase data\n",
    "# But in this case, problem arises when the session's ending time lies betweeen 23:00 and 00:00, and the timestamp of the action in the Purchase data lies between 00:00 and 01:00\n",
    "# Therefore we create \"date2(1 day substractd from the date of the Purchase data)\" column in the Purchase data \n",
    "\n",
    "raw['date'] = raw['datetime'].dt.floor('d')\n",
    "purchase['date'] = purchase['order_ymdt'].dt.floor('d')\n",
    "purchase['date2'] = purchase['order_ymdt'].dt.floor('d') - timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Raw and Purchase data if \"date\" column coincides\n",
    "# Remove data if the timestamp of the user's action does not lie between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "log_purchase = raw.merge(purchase,on=['user_id','date'],how='inner')\n",
    "log_purchase = log_purchase[((log_purchase['order_ymdt'] - log_purchase['begin_time'])>timedelta(0))*((log_purchase['end_time2']-log_purchase['order_ymdt'])>timedelta(0))==1]\n",
    "log_purchase.sort_values(by=['id','datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Raw and Purchase data if \"date\" column of Raw data and \"date2(1 day substractd from the date of the Cart data)\" column of Purchase data coincides\n",
    "# Remove data if the timestamp of the user's action does not lie between the beginning timestamp and the (ending timestamp + 60 minutes) of the session\n",
    "\n",
    "log_purchase2 = raw.merge(purchase,left_on=['user_id','date'],right_on=['user_id','date2'],how='inner')\n",
    "log_purchase2 = log_purchase2[((log_purchase2['order_ymdt'] - log_purchase2['begin_time'])>timedelta(0))*((log_purchase2['end_time2']-log_purchase2['order_ymdt'])>timedelta(0))==1]\n",
    "log_purchase2.sort_values(by=['id','datetime'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate above two data\n",
    "log_purchase = pd.concat([log_purchase,log_purchase2],axis=0)\n",
    "\n",
    "# Match the action in the Purchase data to the nearest (in time) log within the session\n",
    "log_purchase['diff'] = ((log_purchase['order_ymdt'] - log_purchase['datetime']).astype('timedelta64[s]'))**2\n",
    "log_purchase = log_purchase.merge(pd.DataFrame(log_purchase.groupby('id')['diff'].min()).rename(columns={'diff':'min_diff'}),on='id',how='left')\n",
    "log_purchase = log_purchase[(log_purchase['diff']-log_purchase['min_diff'])**2<0.1]\n",
    "log_purchase.drop_duplicates(subset=['id','order_ymdt'],keep='last',inplace=True)\n",
    "log_purchase.drop('min_diff',axis=1,inplace=True)\n",
    "log_purchase['purchase_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34646126547566236"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Only 34% of the Purchase data are matched to the log data \n",
    "# Unmatched data are estimated to be the actions on the web environment (we only used the log data from the mobile environment)\n",
    "\n",
    "log_purchase.shape[0] / purchase.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that were created but no longer useful\n",
    "# Add the suffix \"purchase_\" to the columns created in this section, to avoid the ambiguity\n",
    "\n",
    "log_purchase.drop(['purchase_id','end_time2','date','date2','date_x','date_y'],axis=1,inplace=True)\n",
    "log_purchase.rename(columns={'prod_order_no':'purchase_prod_order_no','order_no':'purchase_order_no','order_ymdt':'purchase_order_ymdt',\n",
    "                                            'prod_no':'purchase_prod_no','prod_nm':'purchase_prod_nm','gmv':'purchase_gmv','flag':'purchase_flag',\n",
    "                                            'catg_nm_l':'purchase_catg_nm_l','catg_nm_m':'purchase_catg_nm_m','catg_nm_s':'purchase_catg_nm_s','diff':'log_purchase_time_diff'},inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Merge the entire Raw, Cart, Like, Purchase data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge log_cart, log_like, log_purchase to Raw\n",
    "# In the precess of merging, drop the columns that the table has in common with the Raw data\n",
    "# (Since we are merging on the \"id\" and \"session_nth\" column, we do not drop these columns even if they are common in two tables)\n",
    "\n",
    "raw_all = raw.merge(log_cart.drop(list((set(log_cart.columns).intersection(set(raw.columns))).difference(set(['id','session_nth']))),axis=1),on=['id','session_nth'],how='left')\n",
    "raw_all = raw_all.merge(log_like.drop(list((set(log_like.columns).intersection(set(raw.columns))).difference(set(['id','session_nth']))),axis=1),on=['id','session_nth'],how='left')\n",
    "raw_all = raw_all.merge(log_purchase.drop(list((set(log_purchase.columns).intersection(set(raw.columns))).difference(set(['id','session_nth']))),axis=1),on=['id','session_nth'],how='left')\n",
    "raw_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"cart_id(1 if there exists at least one Cart action within the session, otherwise 0)\" column\n",
    "# Create \"like_id(1 if there exists at least one Like action within the session, otherwise 0)\" column\n",
    "# Create \"purchase_id(1 if there exists at least one Purchase action within the session, otherwise 0)\" column\n",
    "\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['cart_event_ymdt'].count().gt(0).astype(float)).rename(columns={'cart_event_ymdt':'cart_id'}),on='id',how='left')\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['like_keep_reg_ymdt'].count().gt(0).astype(float)).rename(columns={'like_keep_reg_ymdt':'like_id'}),on='id',how='left')\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['purchase_order_ymdt'].count().gt(0).astype(float)).rename(columns={'purchase_order_ymdt':'purchase_id'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the useless columns from raw_all\n",
    "\n",
    "raw_all.drop(['end_time2','date'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'user_id', 'sex', 'age', 'url', 'device', 'page',\n",
       "       'Labeling', 'session', 'id', 'page_stay_time', 'session_length',\n",
       "       'session_nth', 'prev_Labeling', 'prev_page', 'begin_time', 'end_time',\n",
       "       'cart_prod_no', 'cart_prod_nm', 'cart_event_type', 'cart_event_ymdt',\n",
       "       'cart_catg_nm_l', 'cart_catg_nm_m', 'cart_catg_nm_s',\n",
       "       'log_cart_time_diff', 'like_keep_stat_cd', 'like_prod_no',\n",
       "       'like_prod_nm', 'like_keep_reg_ymdt', 'like_catg_nm_l',\n",
       "       'like_catg_nm_m', 'like_catg_nm_s', 'log_like_time_diff',\n",
       "       'purchase_prod_order_no', 'purchase_order_no', 'purchase_order_ymdt',\n",
       "       'purchase_prod_no', 'purchase_prod_nm', 'purchase_gmv', 'purchase_flag',\n",
       "       'purchase_catg_nm_l', 'purchase_catg_nm_m', 'purchase_catg_nm_s',\n",
       "       'log_purchase_time_diff', 'cart_id', 'like_id', 'purchase_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Remove outlier sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sessions with length less than 2\n",
    "raw_all = raw_all[raw_all['session_length']>2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5003128, 47)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"distinct_page_num(number of distinct pages within the session)\" column\n",
    "# Create the \"distinct_label_num(number of distinct labels within the session)\" column\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['page'].nunique()).rename(columns={'page':'distinct_page_num'}),on='id',how='left')\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['Labeling'].nunique()).rename(columns={'Labeling':'distinct_label_num'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the sessions that consists of identical pages\n",
    "raw_all = raw_all[raw_all['distinct_page_num']>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946055, 49)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create features that describe each session"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Features related to the staying time within each log/session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"max_page_stay_time(maximum staying time of the single log within the session)\" column\n",
    "# Create \"min_page_stay_time(minimum staying time of the single log within the session)\" column\n",
    "\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['page_stay_time'].max()).rename(columns={'page_stay_time':'max_page_stay_time'}),on='id',how='left')\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['page_stay_time'].min()).rename(columns={'page_stay_time':'min_page_stay_time'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"mean_page_stay_time(mean of the staying time for each log within the session)\" column\n",
    "# If we simply apply the \".groupby.mean()\", then the denominators of the mean calculations become inaccurate (substracted by 1)\n",
    "# Therefore we apply \".groupby.count()+1\" instead of \".groupby.sum()\"\n",
    "\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['page_stay_time'].sum() / (raw_all.groupby('id')['page_stay_time'].count()+1)).rename(columns={'page_stay_time':'mean_page_stay_time'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"session_stay_time(total staying time within the session)\" column\n",
    "\n",
    "raw_all['session_stay_time'] = raw_all['end_time'] - raw_all['begin_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of stay_time columns created above are \"timedelta\" \n",
    "# For convenience, convert the data types of these columns into \"float\" of seconds\n",
    "\n",
    "raw_all['page_stay_time'] = raw_all['page_stay_time'].astype('timedelta64[s]')\n",
    "raw_all['max_page_stay_time'] = raw_all['max_page_stay_time'].astype('timedelta64[s]')\n",
    "raw_all['min_page_stay_time'] = raw_all['min_page_stay_time'].astype('timedelta64[s]')\n",
    "raw_all['mean_page_stay_time'] = raw_all['mean_page_stay_time'].astype('timedelta64[s]')\n",
    "raw_all['session_stay_time'] = raw_all['session_stay_time'].astype('timedelta64[s]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Time elapsed from the beginning time of the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the \"time_from_begin(how much time elapsed from the beginning time of the session when the corresponding log occured)\" column\n",
    "\n",
    "raw_all['time_from_begin'] = raw_all['datetime'] - raw_all['begin_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"purchase_page_reach_time(If an user visited the page of Label 21[payment page], how long did it take from the beginning of the session)\" column\n",
    "\n",
    "time_from_begin = raw_all.drop_duplicates(subset=['id','Labeling'])[['id','Labeling','time_from_begin']].rename(columns={'time_from_begin':'purchase_page_reach_time'})\n",
    "raw_all = raw_all.merge(time_from_begin[time_from_begin['Labeling']==21][['id','purchase_page_reach_time']],on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types of above two columns created above are \"timedelta\" \n",
    "# For convenience, convert the data types of these columns into \"float\" of seconds\n",
    "\n",
    "raw_all['time_from_begin'] = raw_all['time_from_begin'].astype('timedelta64[s]')\n",
    "raw_all['purchase_page_reach_time'] = raw_all['purchase_page_reach_time'].astype('timedelta64[s]')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Number of recursion (repetition number of particular label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create \"recursion_4(repetition number of Label 4)\" column\n",
    "\n",
    "recursion_4 = raw_all.groupby(['id','Labeling'])['prev_Labeling'].value_counts().unstack()[4].unstack()[4]\n",
    "raw_all = raw_all.merge(pd.DataFrame(recursion_4).rename(columns={4:'recursion_4'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"recursion_4(repetition number of Label 5)\" column\n",
    "\n",
    "recursion_5 = raw_all.groupby(['id','Labeling'])['prev_Labeling'].value_counts().unstack()[5].unstack()[5]\n",
    "raw_all = raw_all.merge(pd.DataFrame(recursion_5).rename(columns={5:'recursion_5'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"recursion_4(repetition number of Label 6)\" column\n",
    "\n",
    "recursion_6 = raw_all.groupby(['id','Labeling'])['prev_Labeling'].value_counts().unstack()[6].unstack()[6]\n",
    "raw_all = raw_all.merge(pd.DataFrame(recursion_6).rename(columns={6:'recursion_6'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"recursion_4(repetition number of Label 8)\" column\n",
    "\n",
    "recursion_8 = raw_all.groupby(['id','Labeling'])['prev_Labeling'].value_counts().unstack()[8].unstack()[8]\n",
    "raw_all = raw_all.merge(pd.DataFrame(recursion_8).rename(columns={8:'recursion_8'}),on='id',how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Number of Cart / Like / Purchase actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"cart_add_num(Number of Cart actions within the session)\" column\n",
    "# Create \"like_add_num(Number of Like actions within the session)\" column\n",
    "# Create \"purchase_num(Number of Purchase actiosn within the session)\" column\n",
    "\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['cart_event_ymdt'].count()).rename(columns={'cart_event_ymdt':'cart_add_num'}),on='id',how='left')\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['like_keep_reg_ymdt'].count()).rename(columns={'like_keep_reg_ymdt':'like_add_num'}),on='id',how='left')\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['purchase_order_ymdt'].count()).rename(columns={'purchase_order_ymdt':'purchase_num'}),on='id',how='left')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Features related to the particular labels within the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"session_begin_label(beginning label of the session)\" column\n",
    "\n",
    "raw_all = raw_all.merge(raw_all.drop_duplicates(subset='id',keep='first')[['id','Labeling']].rename(columns={'Labeling':'session_begin_label'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"session_end_label(ending label of the session)\" column\n",
    "\n",
    "raw_all = raw_all.merge(raw_all.drop_duplicates(subset='id',keep='last')[['id','Labeling']].rename(columns={'Labeling':'session_end_label'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"review_page_num(how many Label 7[review page] exists within the session)\" columns\n",
    "\n",
    "review_page_num = pd.DataFrame(raw_all.groupby('id')['Labeling'].value_counts().unstack()[7]).rename(columns={7:'review_page_num'})\n",
    "raw_all = raw_all.merge(review_page_num,on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946055, 65)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'user_id', 'sex', 'age', 'url', 'device', 'page',\n",
       "       'Labeling', 'session', 'id', 'page_stay_time', 'session_length',\n",
       "       'session_nth', 'prev_Labeling', 'prev_page', 'begin_time', 'end_time',\n",
       "       'cart_prod_no', 'cart_prod_nm', 'cart_event_type', 'cart_event_ymdt',\n",
       "       'cart_catg_nm_l', 'cart_catg_nm_m', 'cart_catg_nm_s',\n",
       "       'log_cart_time_diff', 'like_keep_stat_cd', 'like_prod_no',\n",
       "       'like_prod_nm', 'like_keep_reg_ymdt', 'like_catg_nm_l',\n",
       "       'like_catg_nm_m', 'like_catg_nm_s', 'log_like_time_diff',\n",
       "       'purchase_prod_order_no', 'purchase_order_no', 'purchase_order_ymdt',\n",
       "       'purchase_prod_no', 'purchase_prod_nm', 'purchase_gmv', 'purchase_flag',\n",
       "       'purchase_catg_nm_l', 'purchase_catg_nm_m', 'purchase_catg_nm_s',\n",
       "       'log_purchase_time_diff', 'cart_id', 'like_id', 'purchase_id',\n",
       "       'distinct_page_num', 'distinct_label_num', 'max_page_stay_time',\n",
       "       'min_page_stay_time', 'mean_page_stay_time', 'session_stay_time',\n",
       "       'time_from_begin', 'purchase_page_reach_time', 'recursion_4',\n",
       "       'recursion_5', 'recursion_6', 'recursion_8', 'cart_add_num',\n",
       "       'like_add_num', 'purchase_num', 'session_begin_label',\n",
       "       'session_end_label', 'review_page_num'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Classifing the inflow type into Search/Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Search inflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"is_query_page(if thw word 'query' exists within the URL)\" column\n",
    "raw_all['is_query_page'] = raw_all['url'].apply(lambda x: 'query' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition to be classified as Search inflow: query_page appears within 30 seconds after the beginning of the session & 2nd log within the session\n",
    "# Create \"under_30sec(if the log is within 30 seconds from the beginning of the session)\" column\n",
    "# Create \"under_2th(if the log is 1st or 2nd log of the session)\" column\n",
    "# Create \"search_in_log(if the log is within 30 seconds from the beginning of the session, and is 1st or 2nd log of the session & and it is query_page)\" column\n",
    "\n",
    "raw_all['under_30sec'] = pd.to_timedelta(raw_all['time_from_begin'],unit='s')<=pd.Timedelta(seconds=30)\n",
    "raw_all['under_2th'] = raw_all['session_nth']<=2\n",
    "raw_all['search_in_log'] = raw_all['under_30sec'] * raw_all['under_2th'] * raw_all['is_query_page']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"search_in(if the session is search inflow session)\" column\n",
    "\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['search_in_log'].sum().gt(0)).astype(float).rename(columns={'search_in_log':'search_in'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns created above\n",
    "\n",
    "raw_all.drop(['under_30sec','under_2th','search_in_log'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946055, 67)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    252550\n",
       "1.0     99357\n",
       "Name: search_in, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.drop_duplicates('id')['search_in'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Exploration inflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Condition to be classified as the Exploration inflow: Except for the Search inflow session, all the other sessions that do not consists solely of transaction(payment)/shipment logs\n",
    "# Create \"is_trans_revisit(whether the log is about transaction or shipment)\" column\n",
    "\n",
    "trans_revisit = [15, 21, 22, 23,17, 18, 19, 20, 25, 26]\n",
    "raw_all['is_trans_revisit'] = raw_all['Labeling'].apply(lambda x: x in trans_revisit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"trans_revisit_sum(number of trans_revisit logs within the session)\" column\n",
    "\n",
    "raw_all = raw_all.merge(pd.DataFrame(raw_all.groupby('id')['is_trans_revisit'].sum()).rename(columns={'is_trans_revisit':'trans_revisit_sum'}),on='id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trans_revisit(whether the session consists solely of transaction(payment)/shipment logs)\" column\n",
    "# When calculating this column, we exclude the Search sessions that consists soly of transaction(payment)/shipment logs\n",
    "\n",
    "raw_all['trans_revisit'] = (raw_all['trans_revisit_sum'] == raw_all['session_length']).astype(float) * (1-raw_all['search_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>search_in</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans_revisit</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>228547.0</td>\n",
       "      <td>99357.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>24003.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "search_in           0.0      1.0\n",
       "trans_revisit                   \n",
       "0.0            228547.0  99357.0\n",
       "1.0             24003.0      NaN"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.drop_duplicates('id').groupby('trans_revisit')['search_in'].value_counts().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns created above\n",
    "\n",
    "raw_all.drop(['is_trans_revisit','trans_revisit_sum'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create \"in_type(to which inflow category the session belongs)\" column\n",
    "\n",
    "def compute_in_type(x):\n",
    "    if x==2:\n",
    "        return '검색유입'\n",
    "    elif x==1:\n",
    "        return '결제/배송'\n",
    "    elif x==0:\n",
    "        return '탐색유입'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "raw_all['in_type'] = (raw_all['search_in']*2 + raw_all['trans_revisit']).apply(lambda x: compute_in_type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>in_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>탐색유입</th>\n",
       "      <td>228547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>검색유입</th>\n",
       "      <td>99357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>결제/배송</th>\n",
       "      <td>24003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       in_type\n",
       "탐색유입    228547\n",
       "검색유입     99357\n",
       "결제/배송    24003"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(raw_all.drop_duplicates('id')['in_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns created above\n",
    "\n",
    "raw_all.drop(['search_in','trans_revisit'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4946055, 67)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['datetime', 'user_id', 'sex', 'age', 'url', 'device', 'page',\n",
       "       'Labeling', 'session', 'id', 'page_stay_time', 'session_length',\n",
       "       'session_nth', 'prev_Labeling', 'prev_page', 'begin_time', 'end_time',\n",
       "       'cart_prod_no', 'cart_prod_nm', 'cart_event_type', 'cart_event_ymdt',\n",
       "       'cart_catg_nm_l', 'cart_catg_nm_m', 'cart_catg_nm_s',\n",
       "       'log_cart_time_diff', 'like_keep_stat_cd', 'like_prod_no',\n",
       "       'like_prod_nm', 'like_keep_reg_ymdt', 'like_catg_nm_l',\n",
       "       'like_catg_nm_m', 'like_catg_nm_s', 'log_like_time_diff',\n",
       "       'purchase_prod_order_no', 'purchase_order_no', 'purchase_order_ymdt',\n",
       "       'purchase_prod_no', 'purchase_prod_nm', 'purchase_gmv', 'purchase_flag',\n",
       "       'purchase_catg_nm_l', 'purchase_catg_nm_m', 'purchase_catg_nm_s',\n",
       "       'log_purchase_time_diff', 'cart_id', 'like_id', 'purchase_id',\n",
       "       'distinct_page_num', 'distinct_label_num', 'max_page_stay_time',\n",
       "       'min_page_stay_time', 'mean_page_stay_time', 'session_stay_time',\n",
       "       'time_from_begin', 'purchase_page_reach_time', 'recursion_4',\n",
       "       'recursion_5', 'recursion_6', 'recursion_8', 'cart_add_num',\n",
       "       'like_add_num', 'purchase_num', 'session_begin_label',\n",
       "       'session_end_label', 'review_page_num', 'is_query_page', 'in_type'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_all.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save as csv file & Save as pickle file after the memory optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire data into the csv file\n",
    "raw_all.to_csv(os.path.join(os.getcwd(),'final_log.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cited from https://data-newbie.tistory.com/472\n",
    "\n",
    "def mem_usage(pandas_obj):\n",
    "    if isinstance(pandas_obj,pd.DataFrame):\n",
    "        usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
    "    else: # we assume if not a df it's a series\n",
    "        usage_b = pandas_obj.memory_usage(deep=True)\n",
    "    usage_mb = usage_b / 1024 ** 2 # convert bytes to megabytes\n",
    "    return \"{:03.2f} MB\".format(usage_mb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9760.81 MB'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage(raw_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cited from https://data-newbie.tistory.com/472\n",
    "\n",
    "def int_memory_reduce(data) :\n",
    "    data_int = data.select_dtypes(include=['int'])\n",
    "    converted_int = data_int.apply(pd.to_numeric,downcast='unsigned')\n",
    "    print(f\"Before : {mem_usage(data_int)} -> After : {mem_usage(converted_int)}\")\n",
    "    data[converted_int.columns] = converted_int\n",
    "    return data\n",
    "\n",
    "## 연속형 데이터 사이즈 축소 함소\n",
    "def float_memory_reduce(data) :\n",
    "    data_float = data.select_dtypes(include=['float'])\n",
    "    converted_float = data_float.apply(pd.to_numeric,downcast='float')\n",
    "    print(f\"Before : {mem_usage(data_float)} -> After : {mem_usage(converted_float)}\")\n",
    "    data[converted_float.columns] = converted_float\n",
    "    return data\n",
    "\n",
    "## 문자형 데이터 사이즈 축소 함소\n",
    "def object_memory_reduce(data) :\n",
    "    gl_obj = data.select_dtypes(include=['object']).copy()\n",
    "    converted_obj = pd.DataFrame()\n",
    "    for col in gl_obj.columns:\n",
    "        num_unique_values = len(gl_obj[col].unique())\n",
    "        num_total_values = len(gl_obj[col])\n",
    "        if num_unique_values / num_total_values < 0.5:\n",
    "            converted_obj.loc[:,col] = gl_obj[col].astype('category')\n",
    "        else:\n",
    "            converted_obj.loc[:,col] = gl_obj[col]\n",
    "    print(f\"Before : {mem_usage(gl_obj)} -> After : {mem_usage(converted_obj)}\")\n",
    "    data[converted_obj.columns] = converted_obj\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5209/1932203267.py:3: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  usage_b = pandas_obj.memory_usage(deep=True).sum()\n",
      "/tmp/ipykernel_5209/1932203267.py:3: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  usage_b = pandas_obj.memory_usage(deep=True).sum()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : 37.74 MB -> After : 37.74 MB\n",
      "Before : 943.39 MB -> After : 490.56 MB\n",
      "Before : 8232.66 MB -> After : 1256.21 MB\n"
     ]
    }
   ],
   "source": [
    "raw_all2 = raw_all.copy()\n",
    "raw_all2 = int_memory_reduce(raw_all2)\n",
    "raw_all2 = float_memory_reduce(raw_all2)\n",
    "raw_all2 = object_memory_reduce(raw_all2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2392.99 MB'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage(raw_all2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all2.to_pickle(os.path.join(os.getcwd(),'final_log.pkl'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Leave only the sessions with length less than 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2948/2580486793.py:1: DtypeWarning: Columns (25,27,28,29,30,31,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  raw_all = pd.read_csv(os.path.join(os.getcwd(),'final_log(cart buy zzim).csv'))\n"
     ]
    }
   ],
   "source": [
    "raw_all = pd.read_csv(os.path.join(os.getcwd(),'final_log.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all_under50 = raw_all[raw_all['session_length']<=50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all_under50.to_csv(os.path.join(os.getcwd(),'final_log_under50.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7547.21 MB'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage(raw_all_under50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before : 359.19 MB -> After : 72.53 MB\n",
      "Before : 690.75 MB -> After : 359.19 MB\n",
      "Before : 6874.22 MB -> After : 1310.79 MB\n"
     ]
    }
   ],
   "source": [
    "raw_all_under50_small = raw_all_under50.copy()\n",
    "raw_all_under50_small = int_memory_reduce(raw_all_under50_small)\n",
    "raw_all_under50_small = float_memory_reduce(raw_all_under50_small)\n",
    "raw_all_under50_small = object_memory_reduce(raw_all_under50_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1690.70 MB'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_usage(raw_all_under50_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_all_under50_small.to_pickle(os.path.join(os.getcwd(),'final_log_under50.pkl'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "preprocess_summary.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jjh1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "15f4aacd9556265f1615cc73c631931044efcc45c198588ea5b6a55eacfa69f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
